"""
This script converts perception@0.8.0-preview.3 package's annotations into COCO 2017 format.
  Ensure that you are generating: RGB images, keypoint annotations, bbox annotations, semantic segmentation,
  and instance segmentation annotations. If your annotations set differs from this, the code below needs to be modified.

The following combinations are allowed:
  {keypoint + bounding box}
  {keypoint + bounding box + semantic segmentation}
  {keypoint + bounding box + instance segmentation}
  {keypoint + bounding box + semantic segmentation + instance segmentation}

Author:
  Salehe

Example usage:
  python convert_perception080_to_coco2017bbox_kpt_instance_segmentation.py \
  -datadir <data-dir-perception>/ \
  -outdir <out-dir>/ \
  -dbtype val \
  -verb 1

"""
import argparse
import glob
import json
import os
import shutil
import sys

import numpy as np
from PIL import Image

# try:
#     from pycocotools.mask import encode as mask_to_rle
#
#     has_pycocotools = True
# except ImportError:
#     print(
#         "[INFO] Could not find pycocotools, cannot store instance segmentation annotations."
#     )
#     has_pycocotools = False
#
# parser = argparse.ArgumentParser(
#     description="Input data directories to convert to COCO."
# )
# parser.add_argument(
#     "-datadir",
#     metavar="datadir",
#     type=str,
#     help="directory containing dataset generated by perception package.",
# )
# parser.add_argument(
#     "-outdir",
#     metavar="outdir",
#     type=str,
#     help="directory to store the coco converted data.",
# )
# parser.add_argument(
#     "-dbtype",
#     metavar="dbtype",
#     type=str,
#     default="train",
#     help="train, test, or val",
# )
# parser.add_argument(
#     "-verb", metavar="verbose", type=int, default="False", help="0 or 1"
# )
#
# args = parser.parse_args()
#
# verbose = args.verb
# datadir = args.datadir
# outdir = args.outdir
#
# if not os.path.isdir(datadir):
#     print("[ERROR] The specified path for datadir does not exist")
#     print("[ERROR] datadir : " + datadir)
#     sys.exit()
# else:
#     print("[INFO]  reading data from {}".format(datadir))
#
#     if glob.glob(datadir + "/Dataset*"):
#         anndirs = glob.glob(datadir + "/Dataset*")
#         for node in range(len(anndirs)):
#             print(
#                 "[INFO]  anndir for node {} : {}".format(
#                     node + 1, anndirs[node]
#                 )
#             )
#     else:
#         print("[ERROR] Annotations do not exist")
#         sys.exit()
#
#     if glob.glob(datadir + "/RGB*"):
#         rgbdirs = glob.glob(datadir + "/RGB*")
#         for node in range(len(rgbdirs)):
#             print(
#                 "[INFO]  rgbdirs for node {} : {}".format(
#                     node + 1, rgbdirs[node]
#                 )
#             )
#     else:
#         print("[ERROR] RGB images do not exist")
#         sys.exit()
#
#     if glob.glob(datadir + "/SemanticSegmentation*"):
#         segdirs = glob.glob(datadir + "/SemanticSegmentation*")
#         for node in range(len(segdirs)):
#             print(
#                 "[INFO]  segdir for node {} : {}".format(
#                     node + 1, segdirs[node]
#                 )
#             )
#         seg = True
#     else:
#         print("[ERROR] Semantic Segmentations do not exist")
#         seg = False
#
#     if glob.glob(datadir + "/InstanceSegmentation*"):
#         insdirs = glob.glob(datadir + "/InstanceSegmentation*")
#         for node in range(len(insdirs)):
#             print(
#                 "[INFO]  insdir for node {} : {}".format(
#                     node + 1, insdirs[node]
#                 )
#             )
#         ins = True
#     else:
#         print("[ERROR] Instance Segmentations do not exist")
#         ins = False
#
#
# category = []
# anndir = anndirs[0]
# annotation_definitions = "annotation_definitions.json"
# with open(anndir + "/" + annotation_definitions) as f:
#     anndef = json.load(f)
#
# """ find indexes for annotation types """
# atd = {}
# for anntype in range(len(anndef["annotation_definitions"])):
#     atd[anndef["annotation_definitions"][anntype]["name"]] = anntype
#
# print(atd)
#
# for lbl in range(1):  # only 1 type of label = person
#     """bounding box"""
#     if "bounding box" in atd.keys():
#         key = anndef["annotation_definitions"][atd["bounding box"]]["spec"][
#             lbl
#         ]["label_id"]
#         val = anndef["annotation_definitions"][atd["bounding box"]]["spec"][
#             lbl
#         ]["label_name"]
#     else:
#         key = "1"
#         val = "person"
#
#     """ semantic segmentation """
#     if seg:
#         pixel_value = anndef["annotation_definitions"][
#             atd["semantic segmentation"]
#         ]["spec"][0][
#             "pixel_value"
#         ]  # {'r': 0.0, 'g': 0.0, 'b': 1.0, 'a': 1.0}
#
#     """ keypoints """
#     if "keypoints" in atd.keys():
#         key_points = []
#         for kp in anndef["annotation_definitions"][atd["keypoints"]]["spec"][0][
#             "key_points"
#         ]:
#             key_points.append(kp["label"])
#         skeleton = []
#         for sk in anndef["annotation_definitions"][atd["keypoints"]]["spec"][0][
#             "skeleton"
#         ]:
#             skeleton.append([sk["joint1"] + 1, sk["joint2"] + 1])
#     else:
#         key_points = []
#         skeleton = []
#
#     category.append(
#         {
#             "supercategory": " _person",
#             "id": key,
#             "name": val,
#             "keypoints": key_points,
#             "skeleton": skeleton,
#         }
#     )
#
# if verbose:
#     print("[DEBUG]  ", category)
#
# db_type = "instances_{}2017".format(args.dbtype)
#
# ann_dest_path = outdir + "/coco/annotations"
# if not os.path.exists(ann_dest_path):
#     os.makedirs(ann_dest_path)
# img_dest_path = outdir + "/coco/images"
# if not os.path.exists(img_dest_path):
#     os.makedirs(img_dest_path)
# img_sub_dest_path = outdir + "/coco/images/" + args.dbtype + "2017/images"
# if not os.path.exists(img_sub_dest_path):
#     os.makedirs(img_sub_dest_path)
#
# aid = 0
# coco = {"images": [], "categories": [], "annotations": []}
#
# total_caps = 0
# total_imgs = 0
# print("[INFO]  reading annotation files.")
#
# mismatch_bbox_kpt = 0
# out_of_bounds_num = 0
# out_of_bounds_corrected_num = 0
# no_annot_num = 0
#
# for node in range(len(anndirs)):
#     print(anndirs)
#
#     anndir = anndirs[node]
#
#     # find all captures_XXX.json files
#     captures = sorted(glob.glob(anndir + "/captures_*.json"))
#
#     for idc, cap in enumerate(range(len(captures))):
#         total_caps += 1
#         if idc == 1:
#             break
#         if verbose:
#             print("[INFO]  reading annotation file {}".format(captures[cap]))
#
#         annot_file = captures[cap]
#         with open(annot_file) as f:
#             data = json.load(f)
#
#         img_num = len(data["captures"])
#
#         for idx, img_id in enumerate(range(img_num)):
#             #print(img_id, "...................")
#
#             if idx == 1:
#                 break
#
#             # if any object is annotated
#             if len(data["captures"][img_id]) > 0:
#
#                 try:
#                     filename = str(data["captures"][img_id]["filename"])
#                     annotations = data["captures"][img_id]["annotations"]
#
#                     """ annotations[X] where X == number of annotation types present """
#                     """ find indexes for annotation types """
#                     atd = {}
#                     for anntype in range(len(annotations)):
#                         # annotations[anntype]['values'][0].keys() exists for bbox, kpt, instance
#                         if "values" in annotations[anntype].keys():
#                             # is it bbox
#                             # print(str(data['captures'][img_id]['filename']))
#                             # print(annotations[anntype]['values'][0].keys())
#                             if (
#                                 "width"
#                                 in annotations[anntype]["values"][0].keys()
#                             ):
#                                 atd["bounding box"] = anntype
#                             # is it instance
#                             elif (
#                                 "color"
#                                 in annotations[anntype]["values"][0].keys()
#                             ):
#                                 atd["instance segmentation"] = anntype
#                             # is it kpt
#                             elif (
#                                 "keypoints"
#                                 in annotations[anntype]["values"][0].keys()
#                             ):
#                                 atd["keypoints"] = anntype
#
#                         # annotations[anntype]['filename'] exists for semantic
#                         elif "filename" in annotations[anntype].keys():
#                             if (
#                                 "SemanticSegmentation"
#                                 in annotations[anntype]["filename"]
#                             ):
#                                 atd["semantic segmentation"] = anntype
#
#                     img = Image.open(os.path.join(datadir, filename))
#                     total_imgs += 1
#                     shutil.copy2(
#                         os.path.join(datadir, filename),
#                         img_sub_dest_path
#                         + "/{}_{}__".format(args.dbtype, node + 1)
#                         + filename.split("/")[1],
#                     )
#                     w, h = img.size
#
#                     img_dict = {
#                         "id": total_imgs,
#                         "file_name": "images/{}_{}__".format(
#                             args.dbtype, node + 1
#                         )
#                         + filename.split("/")[1],
#                         "width": w,
#                         "height": h,
#                     }
#                     coco["images"].append(img_dict)
#                     #print("coming till here&&&&&&&&&&&&")
#                     object_num = len(annotations[atd["bounding box"]]["values"])
#                     kpt_num = len(annotations[atd["keypoints"]]["values"])
#                     ins_num = len(annotations[atd["keypoints"]]["values"])
#                     if verbose:
#                         print(
#                             "[DEBUG]  {} objects visible, {} keypoints annotated in image #{} of node #{}".format(
#                                 object_num, kpt_num, img_id + 1, node + 1
#                             )
#                         )
#                     #print("coming till here&&&&&&&&&&&&")
#                     bbox_instance_id_list = []
#                     kpt_instance_id_list = []
#                     ins_instance_id_list = []
#                     for oid in range(object_num):
#                         bbox_instance_id_list.append(
#                             annotations[atd["bounding box"]]["values"][oid][
#                                 "instance_id"
#                             ]
#                         )
#
#                     for fid in range(kpt_num):
#                         kpt_instance_id_list.append(
#                             annotations[atd["keypoints"]]["values"][fid][
#                                 "instance_id"
#                             ]
#                         )
#                     #print("coming till here&&&&&&&&&&&&")
#                     # for iid in range(ins_num):
#                     #     ins_instance_id_list.append(
#                     #         annotations[atd["instance segmentation"]]["values"][
#                     #             iid
#                     #         ]["instance_id"]
#                     #     )
#                     #print("coming till here&&&&&&&&&&&&")
#                     if "instance segmentation" in atd and has_pycocotools:
#                         ins_filename = annotations[
#                             atd["instance segmentation"]
#                         ]["filename"]
#                         with Image.open(
#                             os.path.join(datadir, ins_filename)
#                         ) as _ins_image:
#                             # _ins_image.show()
#                             if np.shape(_ins_image)[-1] == 4:
#                                 ins_image = np.array(
#                                     _ins_image.getdata(), dtype=np.uint8
#                                 ).reshape(h, w, 4)
#                             else:
#                                 ins_image = np.array(
#                                     _ins_image.getdata(), dtype=np.uint8
#                                 ).reshape(h, w, 3)
#
#                     # print(bbox_instance_id_list, kpt_instance_id_list)
#
#                     for oid in range(object_num):
#
#                         if (
#                             len(annotations[atd["bounding box"]]["values"][oid])
#                             > 0
#                         ):
#
#                             bbox = np.zeros((4))
#
#                             bbox[0] = annotations[atd["bounding box"]][
#                                 "values"
#                             ][oid]["x"]
#                             bbox[1] = annotations[atd["bounding box"]][
#                                 "values"
#                             ][oid]["y"]
#                             bbox[2] = annotations[atd["bounding box"]][
#                                 "values"
#                             ][oid]["width"]
#                             bbox[3] = annotations[atd["bounding box"]][
#                                 "values"
#                             ][oid]["height"]
#
#                             category_id = annotations[atd["bounding box"]][
#                                 "values"
#                             ][oid]["label_id"]
#
#                             current_bbox_instance = annotations[
#                                 atd["bounding box"]
#                             ]["values"][oid]["instance_id"]
#
#                             # --- keypoints ---
#
#                             if current_bbox_instance in kpt_instance_id_list:
#                                 for fid in range(kpt_num):
#                                     current_kpt_instance = annotations[
#                                         atd["keypoints"]
#                                     ]["values"][fid]["instance_id"]
#                                     if (
#                                         current_bbox_instance
#                                         == current_kpt_instance
#                                     ):
#                                         kpt_instance_id = fid
#
#                                 keypoints_vals = []
#                                 num_keypoints = 0
#                                 for kpt in annotations[atd["keypoints"]][
#                                     "values"
#                                 ][kpt_instance_id]["keypoints"]:
#                                     # check if keypoint is out of bounds, truncate if possible
#                                     if (
#                                         kpt["x"] > w
#                                         or kpt["y"] > h
#                                         or kpt["x"] < 0
#                                         or kpt["y"] < 0
#                                     ):  # and kpt['state'] != 0:
#                                         if kpt["x"] > w and kpt["x"] < w + 1:
#                                             kpt["x"] = w
#                                         if kpt["y"] > h and kpt["y"] < h + 1:
#                                             kpt["y"] = h
#                                         if kpt["x"] < 0 and kpt["x"] > 0 - 1:
#                                             kpt["x"] = 0
#                                         if kpt["y"] < 0 and kpt["y"] > 0 - 1:
#                                             kpt["y"] = 0
#
#                                         if (
#                                             kpt["x"] > w
#                                             or kpt["y"] > h
#                                             or kpt["x"] < 0
#                                             or kpt["y"] < 0
#                                         ):
#                                             print(
#                                                 "[ERROR] {}: The specified keypoints are out of bounds.".format(
#                                                     filename.split("/")[1]
#                                                 ),
#                                                 kpt["x"],
#                                                 kpt["y"],
#                                                 kpt["state"],
#                                                 w,
#                                                 h,
#                                             )
#                                             out_of_bounds_num += 1
#                                         else:
#                                             out_of_bounds_corrected_num += 1
#
#                                     # keypoints_vals.append([kpt['x'], kpt['y'], kpt['state']])
#                                     keypoints_vals.append(
#                                         [
#                                             int(np.floor(kpt["x"])),
#                                             int(np.floor(kpt["y"])),
#                                             kpt["state"],
#                                         ]
#                                     )
#                                     if kpt["state"] != 0:
#                                         num_keypoints += 1
#                             else:
#                                 mismatch_bbox_kpt += 1
#                                 keypoints_vals = []
#                                 num_keypoints = 0
#                                 for kpt in range(17):
#                                     # keypoints_vals.append([0.0, 0.0, 0])
#                                     keypoints_vals.append([0, 0, 0])
#
#                             keypoints_vals = [
#                                 item
#                                 for sublist in keypoints_vals
#                                 for item in sublist
#                             ]
#
#                             # --- instance seg ---
#
#                             if (
#                                 current_bbox_instance in ins_instance_id_list
#                                 and has_pycocotools
#                             ):
#                                 # find index in instance segmentation list for the current instance_id
#                                 for iid in range(ins_num):
#                                     current_ins_instance = annotations[
#                                         atd["instance segmentation"]
#                                     ]["values"][iid]["instance_id"]
#                                     if (
#                                         current_bbox_instance
#                                         == current_ins_instance
#                                     ):
#                                         ins_instance_id = iid
#                                         ins_color = annotations[
#                                             atd["instance segmentation"]
#                                         ]["values"][iid]["color"]
#                                         if np.shape(_ins_image)[-1] == 4:
#                                             ins_color = (
#                                                 ins_color["r"],
#                                                 ins_color["g"],
#                                                 ins_color["b"],
#                                                 ins_color["a"],
#                                             )
#                                         else:
#                                             ins_color = (
#                                                 ins_color["r"],
#                                                 ins_color["g"],
#                                                 ins_color["b"],
#                                             )
#                                         break
#
#                                 # Put a 1 everywhere that matches the color, 0 elsewhere
#                                 ins_mask = (
#                                     (ins_image == ins_color)
#                                     .prod(axis=-1)
#                                     .astype(np.uint8)
#                                 )
#
#                                 segmentation = mask_to_rle(
#                                     np.asfortranarray(ins_mask)
#                                 )
#                                 segmentation["counts"] = segmentation[
#                                     "counts"
#                                 ].decode()
#
#                             else:
#                                 segmentation = [
#                                     [
#                                         bbox[0],
#                                         bbox[1],
#                                         bbox[0] + bbox[2],
#                                         bbox[1],
#                                         bbox[0] + bbox[2],
#                                         bbox[1] + bbox[3],
#                                         bbox[0],
#                                         bbox[1] + bbox[3],
#                                     ]
#                                 ]
#
#                             # ---
#
#                             object_dict = {
#                                 "segmentation": segmentation,
#                                 "id": aid,
#                                 "image_id": total_imgs,
#                                 "category_id": category_id,
#                                 "area": bbox[2] * bbox[3],
#                                 "bbox": bbox.tolist(),
#                                 "iscrowd": 0,
#                                 "num_keypoints": num_keypoints,
#                                 "keypoints": keypoints_vals,
#                             }
#
#                             coco["annotations"].append(object_dict)
#                             aid += 1
#
#                 except:
#                     print("image_id", total_imgs, filename.split("/")[1])
#                     print("COULD NOT READ THIS FILE. SKIPPING...")
#                     no_annot_num += 1
#
# coco["categories"] = category
#
# # if verbose:
# #     print('\n\n[DEBUG]  coco annotations json:')
# #     print(coco)
#
# save_path = ann_dest_path + "/" + db_type + ".json"
#
# with open(save_path, "w") as f:
#     json.dump(coco, f)
#
# print(
#     "\n\n[INFO]  Number of discarded images due to no annotation : ",
#     no_annot_num,
# )
# print(
#     "[INFO]  Remaining object instance keypoints out of image bounds : ",
#     out_of_bounds_num,
# )
# print(
#     "[INFO]  Corrected object instance keypoints out of image bounds : ",
#     out_of_bounds_corrected_num,
# )
# print(
#     "[INFO]  Mismatch between bbox instance_id and kpt instance_id : ",
#     mismatch_bbox_kpt,
# )
# print(
#     "\n\n[INFO]  Finished writing coco style annotations for {} images from {} nodes and {} captures.".format(
#         total_imgs, len(anndirs), total_caps
#     )
# )

#############################################


def convert_synthetic_coco(src_data, out_data, dataset_type, verbos):
    sys.argv = [
        "",
        "--datadir",
        src_data,
        "--outdir",
        out_data,
        "--dbtype",
        dataset_type,
        "--verb",
        verbos,
    ]

    try:
        from pycocotools.mask import encode as mask_to_rle

        has_pycocotools = True
    except ImportError:
        print(
            "[INFO] Could not find pycocotools, cannot store instance segmentation annotations."
        )
        has_pycocotools = False

    parser = argparse.ArgumentParser(
        description="Input data directories to convert to COCO."
    )
    parser.add_argument(
        "--datadir",
        metavar="datadir",
        type=str,
        help="directory containing dataset generated by perception package.",
    )
    parser.add_argument(
        "--outdir",
        metavar="outdir",
        type=str,
        help="directory to store the coco converted data.",
    )
    parser.add_argument(
        "--dbtype",
        metavar="dbtype",
        type=str,
        default="train",
        help="train, test, or val",
    )
    parser.add_argument(
        "--verb",
        metavar="verbose",
        type=int,
        default="False",
        required=False,
        help="0 or 1",
    )

    args = parser.parse_args()

    verbose = args.verb
    datadir = args.datadir
    outdir = args.outdir

    if not os.path.isdir(datadir):
        print("[ERROR] The specified path for datadir does not exist")
        print("[ERROR] datadir : " + datadir)
        sys.exit()
    else:
        print("[INFO]  reading data from {}".format(datadir))

        if glob.glob(datadir + "/Dataset*"):
            anndirs = glob.glob(datadir + "/Dataset*")
            for node in range(len(anndirs)):
                print(
                    "[INFO]  anndir for node {} : {}".format(
                        node + 1, anndirs[node]
                    )
                )
        else:
            print("[ERROR] Annotations do not exist")
            sys.exit()

        if glob.glob(datadir + "/RGB*"):
            rgbdirs = glob.glob(datadir + "/RGB*")
            for node in range(len(rgbdirs)):
                print(
                    "[INFO]  rgbdirs for node {} : {}".format(
                        node + 1, rgbdirs[node]
                    )
                )
        else:
            print("[ERROR] RGB images do not exist")
            sys.exit()

        if glob.glob(datadir + "/SemanticSegmentation*"):
            segdirs = glob.glob(datadir + "/SemanticSegmentation*")
            for node in range(len(segdirs)):
                print(
                    "[INFO]  segdir for node {} : {}".format(
                        node + 1, segdirs[node]
                    )
                )
            seg = True
        else:
            print("[ERROR] Semantic Segmentations do not exist")
            seg = False

        if glob.glob(datadir + "/InstanceSegmentation*"):
            insdirs = glob.glob(datadir + "/InstanceSegmentation*")
            for node in range(len(insdirs)):
                print(
                    "[INFO]  insdir for node {} : {}".format(
                        node + 1, insdirs[node]
                    )
                )
            ins = True
        else:
            print("[ERROR] Instance Segmentations do not exist")
            ins = False

    category = []
    anndir = anndirs[0]
    annotation_definitions = "annotation_definitions.json"
    with open(anndir + "/" + annotation_definitions) as f:
        anndef = json.load(f)

    """ find indexes for annotation types """
    atd = {}
    for anntype in range(len(anndef["annotation_definitions"])):
        atd[anndef["annotation_definitions"][anntype]["name"]] = anntype

    print(atd)

    for lbl in range(1):  # only 1 type of label = person
        """bounding box"""
        if "bounding box" in atd.keys():
            key = anndef["annotation_definitions"][atd["bounding box"]]["spec"][
                lbl
            ]["label_id"]
            val = anndef["annotation_definitions"][atd["bounding box"]]["spec"][
                lbl
            ]["label_name"]
        else:
            key = "1"
            val = "person"

        """ semantic segmentation """
        if seg:
            pixel_value = anndef["annotation_definitions"][
                atd["semantic segmentation"]
            ]["spec"][0][
                "pixel_value"
            ]  # {'r': 0.0, 'g': 0.0, 'b': 1.0, 'a': 1.0}

        """ keypoints """
        if "keypoints" in atd.keys():
            key_points = []
            for kp in anndef["annotation_definitions"][atd["keypoints"]][
                "spec"
            ][0]["key_points"]:
                key_points.append(kp["label"])
            skeleton = []
            for sk in anndef["annotation_definitions"][atd["keypoints"]][
                "spec"
            ][0]["skeleton"]:
                skeleton.append([sk["joint1"] + 1, sk["joint2"] + 1])
        else:
            key_points = []
            skeleton = []

        category.append(
            {
                "supercategory": " _person",
                "id": key,
                "name": val,
                "keypoints": key_points,
                "skeleton": skeleton,
            }
        )

    if verbose:
        print("[DEBUG]  ", category)

    db_type = "instances_{}2017".format(args.dbtype)

    ann_dest_path = outdir + "/coco/annotations"
    if not os.path.exists(ann_dest_path):
        os.makedirs(ann_dest_path)
    img_dest_path = outdir + "/coco/images"
    if not os.path.exists(img_dest_path):
        os.makedirs(img_dest_path)
    img_sub_dest_path = outdir + "/coco/images/" + args.dbtype + "2017/images"
    if not os.path.exists(img_sub_dest_path):
        os.makedirs(img_sub_dest_path)

    aid = 0
    coco = {"images": [], "categories": [], "annotations": []}

    total_caps = 0
    total_imgs = 0
    print("[INFO]  reading annotation files.")

    mismatch_bbox_kpt = 0
    out_of_bounds_num = 0
    out_of_bounds_corrected_num = 0
    no_annot_num = 0

    for node in range(len(anndirs)):
        print(anndirs)

        anndir = anndirs[node]

        # find all captures_XXX.json files
        captures = sorted(glob.glob(anndir + "/captures_*.json"))

        for idc, cap in enumerate(range(len(captures))):
            total_caps += 1
            # if idc == 1:
            #     break
            if verbose:
                print(
                    "[INFO]  reading annotation file {}".format(captures[cap])
                )

            annot_file = captures[cap]
            with open(annot_file) as f:
                data = json.load(f)

            img_num = len(data["captures"])

            for idx, img_id in enumerate(range(img_num)):
                # print(img_id, "...................")

                # if idx == 1:
                #     break

                # if any object is annotated
                if len(data["captures"][img_id]) > 0:

                    try:
                        filename = str(data["captures"][img_id]["filename"])
                        annotations = data["captures"][img_id]["annotations"]

                        """ annotations[X] where X == number of annotation types present """
                        """ find indexes for annotation types """
                        atd = {}
                        for anntype in range(len(annotations)):
                            # annotations[anntype]['values'][0].keys() exists for bbox, kpt, instance
                            if "values" in annotations[anntype].keys():
                                # is it bbox
                                # print(str(data['captures'][img_id]['filename']))
                                # print(annotations[anntype]['values'][0].keys())
                                if (
                                    "width"
                                    in annotations[anntype]["values"][0].keys()
                                ):
                                    atd["bounding box"] = anntype
                                # is it instance
                                elif (
                                    "color"
                                    in annotations[anntype]["values"][0].keys()
                                ):
                                    atd["instance segmentation"] = anntype
                                # is it kpt
                                elif (
                                    "keypoints"
                                    in annotations[anntype]["values"][0].keys()
                                ):
                                    atd["keypoints"] = anntype

                            # annotations[anntype]['filename'] exists for semantic
                            elif "filename" in annotations[anntype].keys():
                                if (
                                    "SemanticSegmentation"
                                    in annotations[anntype]["filename"]
                                ):
                                    atd["semantic segmentation"] = anntype

                        img = Image.open(os.path.join(datadir, filename))
                        total_imgs += 1
                        shutil.copy2(
                            os.path.join(datadir, filename),
                            img_sub_dest_path
                            + "/{}_{}__".format(args.dbtype, node + 1)
                            + filename.split("/")[1],
                        )
                        w, h = img.size

                        img_dict = {
                            "id": total_imgs,
                            "file_name": "images/{}_{}__".format(
                                args.dbtype, node + 1
                            )
                            + filename.split("/")[1],
                            "width": w,
                            "height": h,
                        }
                        coco["images"].append(img_dict)
                        # print("coming till here&&&&&&&&&&&&")
                        object_num = len(
                            annotations[atd["bounding box"]]["values"]
                        )
                        kpt_num = len(annotations[atd["keypoints"]]["values"])
                        ins_num = len(annotations[atd["keypoints"]]["values"])
                        if verbose:
                            print(
                                "[DEBUG]  {} objects visible, {} keypoints annotated in image #{} of node #{}".format(
                                    object_num, kpt_num, img_id + 1, node + 1
                                )
                            )
                        # print("coming till here&&&&&&&&&&&&")
                        bbox_instance_id_list = []
                        kpt_instance_id_list = []
                        ins_instance_id_list = []
                        for oid in range(object_num):
                            bbox_instance_id_list.append(
                                annotations[atd["bounding box"]]["values"][oid][
                                    "instance_id"
                                ]
                            )

                        for fid in range(kpt_num):
                            kpt_instance_id_list.append(
                                annotations[atd["keypoints"]]["values"][fid][
                                    "instance_id"
                                ]
                            )
                        # print("coming till here&&&&&&&&&&&&")
                        # for iid in range(ins_num):
                        #     ins_instance_id_list.append(
                        #         annotations[atd["instance segmentation"]]["values"][
                        #             iid
                        #         ]["instance_id"]
                        #     )
                        # print("coming till here&&&&&&&&&&&&")
                        if "instance segmentation" in atd and has_pycocotools:
                            ins_filename = annotations[
                                atd["instance segmentation"]
                            ]["filename"]
                            with Image.open(
                                os.path.join(datadir, ins_filename)
                            ) as _ins_image:
                                # _ins_image.show()
                                if np.shape(_ins_image)[-1] == 4:
                                    ins_image = np.array(
                                        _ins_image.getdata(), dtype=np.uint8
                                    ).reshape(h, w, 4)
                                else:
                                    ins_image = np.array(
                                        _ins_image.getdata(), dtype=np.uint8
                                    ).reshape(h, w, 3)

                        # print(bbox_instance_id_list, kpt_instance_id_list)

                        for oid in range(object_num):

                            if (
                                len(
                                    annotations[atd["bounding box"]]["values"][
                                        oid
                                    ]
                                )
                                > 0
                            ):

                                bbox = np.zeros((4))

                                bbox[0] = annotations[atd["bounding box"]][
                                    "values"
                                ][oid]["x"]
                                bbox[1] = annotations[atd["bounding box"]][
                                    "values"
                                ][oid]["y"]
                                bbox[2] = annotations[atd["bounding box"]][
                                    "values"
                                ][oid]["width"]
                                bbox[3] = annotations[atd["bounding box"]][
                                    "values"
                                ][oid]["height"]

                                category_id = annotations[atd["bounding box"]][
                                    "values"
                                ][oid]["label_id"]

                                current_bbox_instance = annotations[
                                    atd["bounding box"]
                                ]["values"][oid]["instance_id"]

                                # --- keypoints ---

                                if (
                                    current_bbox_instance
                                    in kpt_instance_id_list
                                ):
                                    for fid in range(kpt_num):
                                        current_kpt_instance = annotations[
                                            atd["keypoints"]
                                        ]["values"][fid]["instance_id"]
                                        if (
                                            current_bbox_instance
                                            == current_kpt_instance
                                        ):
                                            kpt_instance_id = fid

                                    keypoints_vals = []
                                    num_keypoints = 0
                                    for kpt in annotations[atd["keypoints"]][
                                        "values"
                                    ][kpt_instance_id]["keypoints"]:
                                        # check if keypoint is out of bounds, truncate if possible
                                        if (
                                            kpt["x"] > w
                                            or kpt["y"] > h
                                            or kpt["x"] < 0
                                            or kpt["y"] < 0
                                        ):  # and kpt['state'] != 0:
                                            if (
                                                kpt["x"] > w
                                                and kpt["x"] < w + 1
                                            ):
                                                kpt["x"] = w
                                            if (
                                                kpt["y"] > h
                                                and kpt["y"] < h + 1
                                            ):
                                                kpt["y"] = h
                                            if (
                                                kpt["x"] < 0
                                                and kpt["x"] > 0 - 1
                                            ):
                                                kpt["x"] = 0
                                            if (
                                                kpt["y"] < 0
                                                and kpt["y"] > 0 - 1
                                            ):
                                                kpt["y"] = 0

                                            if (
                                                kpt["x"] > w
                                                or kpt["y"] > h
                                                or kpt["x"] < 0
                                                or kpt["y"] < 0
                                            ):
                                                print(
                                                    "[ERROR] {}: The specified keypoints are out of bounds.".format(
                                                        filename.split("/")[1]
                                                    ),
                                                    kpt["x"],
                                                    kpt["y"],
                                                    kpt["state"],
                                                    w,
                                                    h,
                                                )
                                                out_of_bounds_num += 1
                                            else:
                                                out_of_bounds_corrected_num += 1

                                        # keypoints_vals.append([kpt['x'], kpt['y'], kpt['state']])
                                        keypoints_vals.append(
                                            [
                                                int(np.floor(kpt["x"])),
                                                int(np.floor(kpt["y"])),
                                                kpt["state"],
                                            ]
                                        )
                                        if kpt["state"] != 0:
                                            num_keypoints += 1
                                else:
                                    mismatch_bbox_kpt += 1
                                    keypoints_vals = []
                                    num_keypoints = 0
                                    for kpt in range(17):
                                        # keypoints_vals.append([0.0, 0.0, 0])
                                        keypoints_vals.append([0, 0, 0])

                                keypoints_vals = [
                                    item
                                    for sublist in keypoints_vals
                                    for item in sublist
                                ]

                                # --- instance seg ---

                                if (
                                    current_bbox_instance
                                    in ins_instance_id_list
                                    and has_pycocotools
                                ):
                                    # find index in instance segmentation list for the current instance_id
                                    for iid in range(ins_num):
                                        current_ins_instance = annotations[
                                            atd["instance segmentation"]
                                        ]["values"][iid]["instance_id"]
                                        if (
                                            current_bbox_instance
                                            == current_ins_instance
                                        ):
                                            ins_instance_id = iid
                                            ins_color = annotations[
                                                atd["instance segmentation"]
                                            ]["values"][iid]["color"]
                                            if np.shape(_ins_image)[-1] == 4:
                                                ins_color = (
                                                    ins_color["r"],
                                                    ins_color["g"],
                                                    ins_color["b"],
                                                    ins_color["a"],
                                                )
                                            else:
                                                ins_color = (
                                                    ins_color["r"],
                                                    ins_color["g"],
                                                    ins_color["b"],
                                                )
                                            break

                                    # Put a 1 everywhere that matches the color, 0 elsewhere
                                    ins_mask = (
                                        (ins_image == ins_color)
                                        .prod(axis=-1)
                                        .astype(np.uint8)
                                    )

                                    segmentation = mask_to_rle(
                                        np.asfortranarray(ins_mask)
                                    )
                                    segmentation["counts"] = segmentation[
                                        "counts"
                                    ].decode()

                                else:
                                    segmentation = [
                                        [
                                            bbox[0],
                                            bbox[1],
                                            bbox[0] + bbox[2],
                                            bbox[1],
                                            bbox[0] + bbox[2],
                                            bbox[1] + bbox[3],
                                            bbox[0],
                                            bbox[1] + bbox[3],
                                        ]
                                    ]

                                # ---

                                object_dict = {
                                    "segmentation": segmentation,
                                    "id": aid,
                                    "image_id": total_imgs,
                                    "category_id": category_id,
                                    "area": bbox[2] * bbox[3],
                                    "bbox": bbox.tolist(),
                                    "iscrowd": 0,
                                    "num_keypoints": num_keypoints,
                                    "keypoints": keypoints_vals,
                                }

                                coco["annotations"].append(object_dict)
                                aid += 1

                    except:
                        print("image_id", total_imgs, filename.split("/")[1])
                        print("COULD NOT READ THIS FILE. SKIPPING...")
                        no_annot_num += 1

    coco["categories"] = category

    # if verbose:
    #     print('\n\n[DEBUG]  coco annotations json:')
    #     print(coco)

    save_path = ann_dest_path + "/" + db_type + ".json"

    with open(save_path, "w") as f:
        json.dump(coco, f)

    print(
        "\n\n[INFO]  Number of discarded images due to no annotation : ",
        no_annot_num,
    )
    print(
        "[INFO]  Remaining object instance keypoints out of image bounds : ",
        out_of_bounds_num,
    )
    print(
        "[INFO]  Corrected object instance keypoints out of image bounds : ",
        out_of_bounds_corrected_num,
    )
    print(
        "[INFO]  Mismatch between bbox instance_id and kpt instance_id : ",
        mismatch_bbox_kpt,
    )
    print(
        "\n\n[INFO]  Finished writing coco style annotations for {} images from {} nodes and {} captures.".format(
            total_imgs, len(anndirs), total_caps
        )
    )
