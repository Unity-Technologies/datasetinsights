{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bounding Box Visualization for SynthDet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is for the model prediction visualization. You can load your models, trained on synthetic datasets generated from the [SynthDet](https://github.com/Unity-Technologies/synthdet) example project, and visualize predicted bounding boxes for the GroceriesReal dataset. Also, this notebook can analyze the easy and hard cases from the point of view of the model. This would provide a better understanding about how to minimize sim2real gaps or improve model performance.<br>\n",
    "You can use this notebook by the following steps:\n",
    "- Specify checkpoint file GCS path. Then, the notebook would load the checkpoints into `FasterRCNN` estimator. `FasterRCNN` can provide model predictions. \n",
    "- Can either specify or randomly select some cases for the visualization. \n",
    "- This notebook would analyze high precision, high recall, low precision, low recall cases for the loaded model.\n",
    "- You can visualize sim2sim prediction results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "from PIL import ImageColor\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from yacs.config import CfgNode as CN\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import torch\n",
    "\n",
    "from datasetinsights.data.simulation.download import Downloader\n",
    "from datasetinsights.data.datasets import SynDetection2D, GroceriesReal\n",
    "from datasetinsights.data.simulation.download import download_manifest\n",
    "from datasetinsights.estimators import Estimator\n",
    "from datasetinsights.estimators.faster_rcnn import FasterRCNN, get_transform, convert_bboxes2canonical\n",
    "from datasetinsights.evaluation_metrics.confusion_matrix import prediction_records, precision_recall\n",
    "from datasetinsights.storage.checkpoint import create_checkpointer\n",
    "from datasetinsights.visualization.plots import grid_plot, plot_bboxes, histogram_plot\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You settings\n",
    "Specify your settings down below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local data root to save downloaded dataset and model:\n",
    "data_root = \"/data\"\n",
    "# GCS Path to model trained on SynthDet dataset:\n",
    "SYNTH_ESTIMATOR_CLOUD_PATH = \"gs://thea-dev/runs/20200615-1751/FasterRCNN.ep7.estimator\"\n",
    "# GCS Path to model trained on GroceriesReal dataset:\n",
    "REAL_ESTIMATOR_CLOUD_PATH = \"gs://thea-dev/runs/faster_rcnn_groceries_v3_20200416_160133/FasterRCNN.ep99.estimator\"\n",
    "# Top/Bottom K images to show according to precision/recall score\n",
    "K = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "Load GrocereisReal data (Validation data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "real_data = GroceriesReal(\n",
    "    data_root=data_root,\n",
    "    split=\"val\",\n",
    "    version=\"v3\",\n",
    "    transforms=get_transform()\n",
    ")\n",
    "print(\"Length of Groceries Real data:\", len(real_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(\"Using cuda\")\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    print(\"Using cpu\")\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def load_estimator(checkpoint):\n",
    "    system = CN()\n",
    "    system.distributed = False\n",
    "\n",
    "    ap_args = CN()\n",
    "    ap_args.iou_threshold = 0.5\n",
    "    ap_args.interpolation = \"NPointInterpolatedAP\"\n",
    "    ap = CN()\n",
    "    ap.name = \"AveragePrecisionBBox2D\"\n",
    "    ap.args = ap_args\n",
    "\n",
    "    ar_args = CN()\n",
    "    ar_args.iou_threshold = 0.5\n",
    "    ar_args.max_detections = 100\n",
    "    ar = CN()\n",
    "    ar.name = \"AverageRecallBBox2D\"\n",
    "    ar.args = ar_args\n",
    "\n",
    "    metrics = CN()\n",
    "    metrics.AP = ap\n",
    "    metrics.AR = ar\n",
    "\n",
    "    config = CN()\n",
    "    config.pretrained = False\n",
    "    config.backbone = \"resnet50\"\n",
    "    config.num_classes = 64\n",
    "    config.checkpoint_file = checkpoint\n",
    "    config.estimator = \"FasterRCNN\"\n",
    "    config.pretrained_backbone = True\n",
    "    config.system = system\n",
    "    config.metrics = metrics\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    checkpointer = create_checkpointer(logdir=writer.logdir, config=config)\n",
    "    estimator = Estimator.create(config.estimator, config=config, writer=writer, device=device, checkpointer=checkpointer)\n",
    "    \n",
    "    return estimator\n",
    "\n",
    "synth_estimator = load_estimator(SYNTH_ESTIMATOR_CLOUD_PATH)\n",
    "real_estimator = load_estimator(REAL_ESTIMATOR_CLOUD_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIM2REAL Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use estimators train on SynthDet dataset and predict on GroceriesReal Dataset, we use this test to:<br>\n",
    "- identify/visualize sim2real gaps.\n",
    "- analyze easy and hard cases from the point of view of the model.<br>\n",
    "<br>We highlight predicted bounding boxes into 2 colors: Green and Red.<br>\n",
    "<font color='green'>Green boxes</font>: it's a correct predicted label and overlap the true box enough (overlap >= 0.5). <br>\n",
    "<font color='red'>Red boxes</font>: it's a wrong prediction.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(synth_estimator, dataset, device, iou_thresh=0.5, box_score_thresh=0.5, real_estimator=None, index=None):\n",
    "    \"\"\" model prediction.\n",
    "\n",
    "    Args:\n",
    "        synth_estimator (FasterRCNN): sim2real model.\n",
    "        dataset: Dataset for prediction.\n",
    "        device: model training on device (cpu|cuda)\n",
    "        iou_thresh (float): iou threshold. Defaults to 0.5.\n",
    "        box_score_thresh (float): box score threshold for filter out lower score bounding boxes. Defaults to 0.5.\n",
    "        real_estimator (FasterRCNN): real2real model. Defaults to None.\n",
    "        index (int): image index used for prediction. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        a list of annotations.\n",
    "    \"\"\"\n",
    "    if index is None:\n",
    "        index = random.randint(0, len(dataset) - 1)\n",
    "        print(\"Data index: \", index)\n",
    "    pil_img, annotation = dataset[index]\n",
    "    annotation = convert_bboxes2canonical([annotation])[0]\n",
    "    img = transforms.ToPILImage()(pil_img).convert(\"RGB\")\n",
    "    \n",
    "    synth_predict_annotation = synth_estimator.predict(pil_img, box_score_thresh=box_score_thresh)\n",
    "    if real_estimator:\n",
    "        real_predict_annotation = real_estimator.predict(pil_img, box_score_thresh=box_score_thresh)\n",
    "        return img, annotation, synth_predict_annotation, real_predict_annotation\n",
    "    else:\n",
    "        return img, annotation, synth_predict_annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLOR_MAPPING = (ImageColor.getcolor(\"red\", \"RGB\"), ImageColor.getcolor(\"green\", \"RGB\"))\n",
    "def render_color_boxes(image, annotation, predict_annotation, line_width=10, font_scale=50):\n",
    "    \"\"\" render color for each box in one image.\n",
    "\n",
    "    Args:\n",
    "        image (PIL Image): the image for rendering.\n",
    "        annotation (list[BBox2D]): ground truth annotation.\n",
    "        predict_annotation (list[BBox2D]): predicted annotation.\n",
    "        line_width (float): line width of the bounding boxes. Defaults to 10.\n",
    "        font_scale (int): how many chars can be filled in the image horizontally. Defaults to 50.\n",
    "\n",
    "    Returns:\n",
    "        a rendered image.\n",
    "    \"\"\"\n",
    "    pred_infos = prediction_records(annotation, predict_annotation)\n",
    "    colors = [COLOR_MAPPING[rec] for score, rec in pred_infos]\n",
    "    pred_img = plot_bboxes(\n",
    "        image,\n",
    "        predict_annotation,\n",
    "        colors=colors,\n",
    "        box_line_width=line_width,\n",
    "        font_scale=font_scale,\n",
    "    )\n",
    "    return pred_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(image_indicies):\n",
    "    \"\"\" visualize predictions for SynthDet model (or real2real model).\n",
    "\n",
    "    Args:\n",
    "        image_indicies (list[int]): image indices for the visualization.\n",
    "\n",
    "    \"\"\"\n",
    "    for i in image_indicies:\n",
    "        img, annotation, synth_pred_ann, real_pred_ann = prediction(\n",
    "            synth_estimator,\n",
    "            real_data,\n",
    "            device,\n",
    "            iou_thresh=0.5,\n",
    "            real_estimator=real_estimator,\n",
    "            index=i,\n",
    "        )\n",
    "        gt_img = plot_bboxes(img, annotation, box_line_width=15, font_scale=50)\n",
    "        synth_pred_img = render_color_boxes(img, annotation, synth_pred_ann)\n",
    "        real_pred_img = render_color_boxes(img, annotation, real_pred_ann)\n",
    "        titles = [\n",
    "            f\"ground truth bounding boxes for Img {i + 1}\",\n",
    "            f\"sim2real prediction for Img {i + 1}\",\n",
    "            f\"ground truth bounding boxes for Img {i + 1}\",\n",
    "            f\"real2real prediction for Img {i + 1}\",\n",
    "        ]\n",
    "        grid_plot([[gt_img, synth_pred_img], [gt_img, real_pred_img]], figsize=(10, 10), img_type=\"rgb\", titles=titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Label mapping table</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_mapping_infos = pd.DataFrame(real_data.label_mappings.items(), columns=[\"Label ID\", \"Label Name\"])\n",
    "label_mapping_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Plot description</b><br>\n",
    "For each image, there are four plots: <br>\n",
    "- The first row is the ground truth(left) and sim2real model prediction(right). <br>\n",
    "- The second row is the ground truth(left) and real2real model prediction(right).  <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predictions for some user-selected cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For bounding box visualization, You can specify some cases or randomly select K cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify image_indicies if you want to visualize a particular set of images\n",
    "# image_indicies = [0, 1, 2]\n",
    "image_indicies = np.random.randint(low=0, high=len(real_data), size=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(image_indicies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization by precision/recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the prediction per image according to precision/recall. Here we define the precision/recall per image as follows:\n",
    "\n",
    "-  precision per image = TP / (TP + FP).\n",
    "-  recall per image = TP / (TP + FN). \n",
    "\n",
    "Here True Positive (TP) are defined as the the bounding boxes with IOU > threshold. We choose threshold = 0.5.\n",
    "This preidcition is class agnostic. It's computed per image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Per image precision and recall distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_all_pr(\n",
    "    model,\n",
    "    dataset,\n",
    "    rate_records,\n",
    "    iou_thresh=0.5,\n",
    "):\n",
    "    \n",
    "    \"\"\" calculate precision and recall for all images.\n",
    "        \n",
    "    Args:\n",
    "        model (FasterRCNN): a faster_rcnn model.\n",
    "        dataset: Dataset for prediction.\n",
    "        iou_thresh (float): iou threshold. Defaults to 0.5.\n",
    "        rate_records (pandas.DataFrame): prediction records. It has three columns, id, precision, recall\n",
    "\n",
    "    \"\"\"\n",
    "    for i in range(len(real_data)):\n",
    "        pil_img, annotation = dataset[i]\n",
    "        annotation = convert_bboxes2canonical([annotation])[0]\n",
    "        predict_annotation = model.predict(pil_img, box_score_thresh=0.5)\n",
    "        ap, ar = precision_recall(annotation, predict_annotation, iou_thresh)\n",
    "        rate_records.loc[len(rate_records)] = [i, ap, ar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate precison recall for all images and all prediction\n",
    "rate_records = pd.DataFrame({\n",
    "    \"id\": pd.Series([], dtype=\"int\"),\n",
    "    \"precision\": pd.Series([], dtype=\"float\"),\n",
    "    \"recall\": pd.Series([], dtype=\"float\")\n",
    "})\n",
    "calculate_all_pr(synth_estimator, real_data, rate_records, iou_thresh=0.5)\n",
    "rate_records[\"id\"] = rate_records[\"id\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot(\n",
    "    rate_records, \n",
    "    x=\"precision\",  \n",
    "    x_title=\"precision\",\n",
    "    y_title=\"Frequency\",\n",
    "    title=f\"per image precision dostribution\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histogram_plot(\n",
    "    rate_records, \n",
    "    x=\"recall\",  \n",
    "    x_title=\"Recall per image\",\n",
    "    y_title=\"Frequency\",\n",
    "    title=f\"Histogram of recall per image for {len(rate_records)} images\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following section visualize four cases \n",
    "\n",
    "We pick K images from the following regions:\n",
    "\n",
    "- [High Precision](#high_precision)\n",
    "- [High Recall](#high_recall)\n",
    "- [Low Precision](#low_precision)\n",
    "- [Low Recall](#low_recall) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='high_precision'></a>\n",
    "#### High Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "precision_easy_cases = rate_records.nlargest(K, \"precision\")\n",
    "precision_easy_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(precision_easy_cases[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='high_recall'></a>\n",
    "#### High Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "recall_easy_cases = rate_records.nlargest(K, 'recall')\n",
    "recall_easy_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(recall_easy_cases[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='low_precision'></a>\n",
    "#### Low Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "precision_hard_cases = rate_records.nsmallest(K, 'precision')\n",
    "precision_hard_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(precision_hard_cases[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='low_recall'></a>\n",
    "#### Low Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "recall_hard_cases = rate_records.nsmallest(K, 'recall')\n",
    "recall_hard_cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_predictions(recall_hard_cases[\"id\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SIM2SIM Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use Synthetic-trained model to predict Synthetic data. Randomly select K Synthetic images. For each image, there are two plots:\n",
    "\n",
    "- The ground truth(left). <br>\n",
    "- The sim2real model prediction(right). <br>\n",
    "\n",
    "This section requires to download the whole Synthetic dataset. Please make sure you have enough disk. You can get auth_token by following this [tutorial](https://github.com/Unity-Technologies/Unity-Simulation-Docs/blob/master/doc/quickstart.md)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run execution id:\n",
    "run_execution_id = \"xxx\"\n",
    "# auth token:\n",
    "auth_token = \"xxx\"\n",
    "# annotation definition id:\n",
    "annotation_definition_id = \"6716c783-1c0e-44ae-b1b5-7f068454b66e\"\n",
    "# unity project id\n",
    "project_id = \"xxx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and load Synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "manifest_file = os.path.join(data_root, \"synthetic\", f\"{run_execution_id}.csv\")\n",
    "download_manifest(run_execution_id, manifest_file, auth_token, project_id)\n",
    "dl = Downloader(manifest_file, data_root, use_cache=True)\n",
    "dl.download_references()\n",
    "dl.download_binary_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "synth_data = SynDetection2D(\n",
    "    data_root=data_root,\n",
    "    manifest_file=manifest_file,\n",
    "    def_id=annotation_definition_id,\n",
    "    transforms=get_transform()\n",
    ")\n",
    "print(\"Length of Synth data:\", len(synth_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can specify some cases or randomly select K cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# specify image_indicies if you want to visualize a particular set of images\n",
    "# synth_image_indicies = [0, 1, 2]\n",
    "synth_image_indicies = np.random.randint(low=0, high=len(synth_data), size=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in synth_image_indicies:\n",
    "    img, annotation, synth_pred_ann = prediction(\n",
    "        synth_estimator,\n",
    "        synth_data,\n",
    "        device,\n",
    "        iou_thresh=0.5,\n",
    "        index=i,\n",
    "    )\n",
    "    gt_img = plot_bboxes(img, annotation, box_line_width=2, font_scale=50)\n",
    "    synth_pred_img = render_color_boxes(img, annotation, synth_pred_ann, line_width=2)\n",
    "    titles = [\n",
    "        f\"ground truth bounding boxes for Img {i + 1}\",\n",
    "        f\"sim2sim prediction for Img {i + 1}\",\n",
    "    ]\n",
    "    grid_plot([[gt_img, synth_pred_img]], figsize=(10, 10), img_type=\"rgb\", titles=titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
