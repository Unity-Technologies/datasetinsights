{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Statistics for Perception Package Projects\n",
    "This example notebook shows how to use datasetinsights to load synthetic datasets generated from the [Perception package](https://github.com/Unity-Technologies/com.unity.perception) and visualize dataset statistics. It includes statistics and visualizations of the outputs built into the Perception package and should give a good idea of how to use datasetinsights to visualize custom annotations and metrics.\n",
    "\n",
    "## Setup dataset\n",
    "If the dataset was generated locally, point `data_root` below to the path of the dataset. The `GUID` folder suffix should be changed accordingly.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/data/<GUID>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Simulation [Optional]\n",
    "If the dataset was generated on Unity Simulation, the following cells can be used to download the metrics needed for dataset statistics.\n",
    "\n",
    "Provide the `run-execution-id` which generated the dataset and a valid `auth_token` in the following cell. The `auth-token` can be generated using the Unity Simulation [CLI](https://github.com/Unity-Technologies/Unity-Simulation-Docs/blob/master/doc/cli.md#usim-inspect-auth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# from datasetinsights.data.simulation.download import download_manifest, Downloader\n",
    "# data_volume = \"/data\"   # directory where datasets should be downloaded to and loaded from\n",
    "# run_execution_id = \"ojEawoj\"      # Unity Simulation run-execution-id\n",
    "# auth_token = \"xxxx\"     # Unity Simulation auth token\n",
    "# project_id = \"xxxx\"   # Unity Project ID\n",
    "\n",
    "# data_root = os.path.join(data_volume, run_execution_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the dataset metadata for statistics we first download the relevant files from Unity Simulation. For downloading files, Unity Simulation provides a manifest file providing file paths and signed urls for each file. `download_manifest()` will download the manifest file to disk. `Download` can then be used to download the metrics and metric definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manifest_file = os.path.join(data_volume, f\"{run_execution_id}.csv\")\n",
    "# download_manifest(run_execution_id, manifest_file, auth_token, project_id)\n",
    "\n",
    "# dl = Downloader(manifest_file, data_root, use_cache=True)\n",
    "# dl.download_references()\n",
    "# dl.download_metrics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset metadata\n",
    "Once the dataset metadata is downloaded, it can be loaded for statistics using `datasetinsights.data.simulation`. Annotation and metric definitions are loaded into pandas dataframes using `AnnotationDefinitions` and `MetricDefinitions` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasetinsights.data.simulation as sim\n",
    "ann_def = sim.AnnotationDefinitions(data_root)\n",
    "ann_def.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_def = sim.MetricDefinitions(data_root)\n",
    "metric_def.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Statistics\n",
    "The following tables and charts are supplied by `datasetinsights.data.datasets.statistics.RenderedObjectInfo` on datasets that include the \"rendered object info\" metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasetinsights.data.datasets.statistics as stat\n",
    "import datasetinsights.data.simulation.metrics as metrics\n",
    "from datasetinsights.data.simulation.exceptions import DefinitionIDError\n",
    "from datasetinsights.visualization.plots import bar_plot, histogram_plot, rotation_plot\n",
    "\n",
    "max_samples = 10000          # maximum number of samples points used in histogram plots\n",
    "\n",
    "rendered_object_info_definition_id = \"5ba92024-b3b7-41a7-9d3f-c03a6a8ddd01\"\n",
    "\n",
    "roinfo = None\n",
    "try:\n",
    "    roinfo = stat.RenderedObjectInfo(data_root=data_root, def_id=rendered_object_info_definition_id)\n",
    "except DefinitionIDError:\n",
    "    print(\"No RenderedObjectInfo in this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    print(roinfo.num_captures())\n",
    "    roinfo.raw_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Object Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    total_count = roinfo.total_counts()\n",
    "    display(total_count)\n",
    "    \n",
    "    display(bar_plot(\n",
    "        total_count, \n",
    "        x=\"label_id\", \n",
    "        y=\"count\", \n",
    "        x_title=\"Label Name\",\n",
    "        y_title=\"Count\",\n",
    "        title=\"Total Object Count in Dataset\",\n",
    "        hover_name=\"label_name\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Capture Object Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    per_capture_count = roinfo.per_capture_counts()\n",
    "    display(per_capture_count.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    display(histogram_plot(\n",
    "        per_capture_count, \n",
    "        x=\"count\",  \n",
    "        x_title=\"Object Counts Per Capture\",\n",
    "        y_title=\"Frequency\",\n",
    "        title=\"Distribution of Object Counts Per Capture\",\n",
    "        max_samples=max_samples\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Visible Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    display(histogram_plot(\n",
    "        roinfo.raw_table, \n",
    "        x=\"visible_pixels\",  \n",
    "        x_title=\"Visible Pixels Per Object\",\n",
    "        y_title=\"Frequency\",\n",
    "        title=\"Distribution of Visible Pixels Per Object\",\n",
    "        max_samples=max_samples\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Visualization\n",
    "In the following sections we show how to load annotations from the Captures object and visualize them. Similar code can be used to consume annotations for model training or visualize and train on custom annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Simulation [Optional]\n",
    "If the dataset was generated on Unity Simulation, the following cells can be used to download the images, captures and annotations in the dataset. Make sure you have enough disk space to store all files. For example, a dataset with 100K captures requires roughly 300GiB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dl.download_captures()\n",
    "# dl.download_binary_files()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = sim.Captures(data_root)\n",
    "cap.captures.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Boxes\n",
    "In this section we render 2d bounding boxes on top of the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from PIL import Image\n",
    "from datasetinsights.visualization.plots import plot_bboxes\n",
    "from datasetinsights.data.datasets.synthetic import read_bounding_box_2d\n",
    "\n",
    "bounding_box_definition_id = \"f9f22e05-443f-4602-a422-ebe4ea9b55cb\"\n",
    "line_width = 5\n",
    "def draw_bounding_boxes(index):\n",
    "    filename = os.path.join(data_root, box_captures.loc[index, \"filename\"])\n",
    "    annotations = box_captures.loc[index, \"annotation.values\"]\n",
    "    image = Image.open(filename)\n",
    "    boxes = read_bounding_box_2d(annotations)\n",
    "    img_with_boxes = plot_bboxes(image, boxes, box_line_width=line_width)\n",
    "    img_with_boxes.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_boxes)\n",
    "\n",
    "try:\n",
    "    box_captures = cap.filter(def_id=bounding_box_definition_id)\n",
    "    interact(draw_bounding_boxes, index=(0, box_captures.shape[0]))\n",
    "except DefinitionIDError:\n",
    "    print(\"No bounding boxes found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "In this section we render the semantic segmentation images on top of the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_with_segmentation(index, opacity):\n",
    "    filename = os.path.join(data_root, seg_captures.loc[index, \"filename\"])\n",
    "    seg_filename = os.path.join(data_root, seg_captures.loc[index, \"annotation.filename\"])\n",
    "    \n",
    "    image = Image.open(filename)\n",
    "    seg = Image.open(seg_filename)\n",
    "    img_with_seg = Image.blend(image, seg, opacity)\n",
    "    img_with_seg.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_seg)\n",
    "    \n",
    "try:\n",
    "    semantic_segmentation_definition_id = \"12f94d8d-5425-4deb-9b21-5e53ad957d66\"\n",
    "    seg_captures = cap.filter(def_id=semantic_segmentation_definition_id)\n",
    "    interact(draw_with_segmentation, index=(0, seg_captures.shape[0]), opacity=(0.0, 1.0))\n",
    "except DefinitionIDError:\n",
    "    print(\"No semantic segmentation images found\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
