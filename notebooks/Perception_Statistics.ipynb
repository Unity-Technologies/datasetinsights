{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Statistics for Perception Package Projects\n",
    "This example notebook shows how to use datasetinsights to load synthetic datasets generated from the [Perception package](https://github.com/Unity-Technologies/com.unity.perception) and visualize dataset statistics. It includes statistics and visualizations of the outputs built into the Perception package and should give a good idea of how to use datasetinsights to visualize custom annotations and metrics.\n",
    "\n",
    "## Setup dataset\n",
    "If the dataset was generated locally, point `data_root` below to the path of the dataset. The `GUID` folder suffix should be changed accordingly.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/data/<GUID>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Simulation [Optional]\n",
    "If the dataset was generated on Unity Simulation, the following cells can be used to download the metrics needed for dataset statistics.\n",
    "\n",
    "Provide the `run-execution-id` which generated the dataset and a valid `access_token` in the following cell. The `access_token` can be generated using the Unity Simulation [CLI](https://github.com/Unity-Technologies/Unity-Simulation-Docs/blob/master/doc/cli.md#usim-inspect-auth)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.io.downloader import UnitySimulationDownloader\n",
    "\n",
    "#run execution id:\n",
    "# run_execution_id = \"xxx\"\n",
    "# #access_token:\n",
    "# access_token = \"xxx\"\n",
    "# #annotation definition id:\n",
    "# annotation_definition_id = \"6716c783-1c0e-44ae-b1b5-7f068454b66e\"\n",
    "# #unity project id\n",
    "# project_id = \"xxx\"\n",
    "# source_uri = f\"usim://{project_id}/{run_execution_id}\"\n",
    "\n",
    "# downloader = UnitySimulationDownloader(access_token=access_token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before loading the dataset metadata for statistics we first download the relevant files from Unity Simulation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloader.download(source_uri=source_uri, output=data_root)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load dataset metadata\n",
    "Once the dataset metadata is downloaded, it can be loaded for statistics using `datasetinsights.data.simulation`. Annotation and metric definitions are loaded into pandas dataframes using `AnnotationDefinitions` and `MetricDefinitions` respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.datasets.unity_perception import AnnotationDefinitions, MetricDefinitions\n",
    "ann_def = AnnotationDefinitions(data_root)\n",
    "ann_def.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_def = MetricDefinitions(data_root)\n",
    "metric_def.table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Built-in Statistics\n",
    "The following tables and charts are supplied by `datasetinsights.data.datasets.statistics.RenderedObjectInfo` on datasets that include the \"rendered object info\" metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.stats.statistics import RenderedObjectInfo\n",
    "import datasetinsights.datasets.unity_perception.metrics as metrics\n",
    "from datasetinsights.datasets.unity_perception.exceptions import DefinitionIDError\n",
    "from datasetinsights.stats import bar_plot, histogram_plot, rotation_plot\n",
    "\n",
    "max_samples = 10000          # maximum number of samples points used in histogram plots\n",
    "\n",
    "rendered_object_info_definition_id = \"5ba92024-b3b7-41a7-9d3f-c03a6a8ddd01\"\n",
    "roinfo = None\n",
    "try:\n",
    "    roinfo = RenderedObjectInfo(data_root=data_root, def_id=rendered_object_info_definition_id)\n",
    "except DefinitionIDError:\n",
    "    print(\"No RenderedObjectInfo in this dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    print(roinfo.num_captures())\n",
    "    roinfo.raw_table.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total Object Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    total_count = roinfo.total_counts()\n",
    "    display(total_count)\n",
    "    \n",
    "    display(bar_plot(\n",
    "        total_count, \n",
    "        x=\"label_id\", \n",
    "        y=\"count\", \n",
    "        x_title=\"Label Name\",\n",
    "        y_title=\"Count\",\n",
    "        title=\"Total Object Count in Dataset\",\n",
    "        hover_name=\"label_name\"\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Per Capture Object Count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    per_capture_count = roinfo.per_capture_counts()\n",
    "    display(per_capture_count.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    display(histogram_plot(\n",
    "        per_capture_count, \n",
    "        x=\"count\",  \n",
    "        x_title=\"Object Counts Per Capture\",\n",
    "        y_title=\"Frequency\",\n",
    "        title=\"Distribution of Object Counts Per Capture\",\n",
    "        max_samples=max_samples\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Object Visible Pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if roinfo is not None:\n",
    "    display(histogram_plot(\n",
    "        roinfo.raw_table, \n",
    "        x=\"visible_pixels\",  \n",
    "        x_title=\"Visible Pixels Per Object\",\n",
    "        y_title=\"Frequency\",\n",
    "        title=\"Distribution of Visible Pixels Per Object\",\n",
    "        max_samples=max_samples\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation Visualization\n",
    "In the following sections we show how to load annotations from the Captures object and visualize them. Similar code can be used to consume annotations for model training or visualize and train on custom annotations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unity Simulation [Optional]\n",
    "If the dataset was generated on Unity Simulation, the following cells can be used to download the images, captures and annotations in the dataset. Make sure you have enough disk space to store all files. For example, a dataset with 100K captures requires roughly 300GiB storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloader.download(source_uri=source_uri, output=data_root, include_binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load captures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetinsights.datasets.unity_perception.captures import Captures\n",
    "cap = Captures(data_root)\n",
    "cap.captures.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bounding Boxes\n",
    "In this section we render 2d bounding boxes on top of the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "from PIL import Image\n",
    "from datasetinsights.stats.visualization.plots import plot_bboxes\n",
    "from datasetinsights.datasets.synthetic import read_bounding_box_2d\n",
    "\n",
    "bounding_box_definition_id = \"f9f22e05-443f-4602-a422-ebe4ea9b55cb\"\n",
    "def draw_bounding_boxes(index):\n",
    "    filename = os.path.join(data_root, box_captures.loc[index, \"filename\"])\n",
    "    annotations = box_captures.loc[index, \"annotation.values\"]\n",
    "\n",
    "    label_mapping = dict()\n",
    "    for i in ann_def.get_definition(bounding_box_definition_id)['spec']:\n",
    "        label_mapping[i['label_id']] = i['label_name']\n",
    "\n",
    "    image = Image.open(filename)\n",
    "    boxes = read_bounding_box_2d(annotations)\n",
    "    img_with_boxes = plot_bboxes(image, boxes, label_mappings=label_mapping)\n",
    "    img_with_boxes.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_boxes)\n",
    "\n",
    "try:\n",
    "    box_captures = cap.filter(def_id=bounding_box_definition_id)\n",
    "    interact(draw_bounding_boxes, index=(0, box_captures.shape[0]))\n",
    "except DefinitionIDError:\n",
    "    print(\"No bounding boxes found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3D Ground Truth Bounding Boxes\n",
    "In this section we render 3d ground truth bounding boxes on top of the captured images."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "from ipywidgets import interact\n",
    "from PIL import Image\n",
    "from datasetinsights.stats.visualization.plots import plot_bboxes3d\n",
    "from datasetinsights.datasets.synthetic import read_bounding_box_3d\n",
    "\n",
    "bounding_box_3d_defintion_id = \"0bfbe00d-00fa-4555-88d1-471b58449f5c\"\n",
    "def draw_bounding_boxes3d(index):\n",
    "    filename = os.path.join(data_root, box_captures.loc[index, \"filename\"])\n",
    "    annotations = box_captures.loc[index, \"annotation.values\"]\n",
    "    sensor = box_captures.loc[index, \"sensor\"]\n",
    "\n",
    "    if 'camera_intrinsic' in sensor:\n",
    "        projection = np.array(sensor[\"camera_intrinsic\"])\n",
    "    else:\n",
    "        projection = np.array([[1,0,0],[0,1,0],[0,0,1]])\n",
    "\n",
    "    image = Image.open(filename)\n",
    "    boxes = read_bounding_box_3d(annotations)\n",
    "    img_with_boxes = plot_bboxes3d(image, boxes, projection)\n",
    "    img_with_boxes.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_boxes)\n",
    "\n",
    "try:\n",
    "    box_captures = cap.filter(def_id=bounding_box_3d_defintion_id)\n",
    "    interact(draw_bounding_boxes3d, index=(0, box_captures.shape[0]))\n",
    "except DefinitionIDError:\n",
    "    print(\"No bounding boxes found\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic Segmentation\n",
    "In this section we render the semantic segmentation images on top of the captured images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_with_segmentation(index, opacity):\n",
    "    filename = os.path.join(data_root, seg_captures.loc[index, \"filename\"])\n",
    "    seg_filename = os.path.join(data_root, seg_captures.loc[index, \"annotation.filename\"])\n",
    "    \n",
    "    image = Image.open(filename)\n",
    "    seg = Image.open(seg_filename)\n",
    "    img_with_seg = Image.blend(image, seg, opacity)\n",
    "    img_with_seg.thumbnail([1024,1024], Image.ANTIALIAS)\n",
    "    display(img_with_seg)\n",
    "    \n",
    "try:\n",
    "    semantic_segmentation_definition_id = \"12f94d8d-5425-4deb-9b21-5e53ad957d66\"\n",
    "    seg_captures = cap.filter(def_id=semantic_segmentation_definition_id)\n",
    "    interact(draw_with_segmentation, index=(0, seg_captures.shape[0]), opacity=(0.0, 1.0))\n",
    "except DefinitionIDError:\n",
    "    print(\"No semantic segmentation images found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
